<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Rachel Gardner</title>
  
  <meta name="author" content="Rachel Gardner">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/stanford_logo.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Rachel Gardner</name>
              </p>
              <p>I am a master's student at Stanford University, where I work on robotics and computer vision.
              </p>
              <p style="text-align:center">
                <a href="https://scholar.google.com/citations?user=9d-HAVMAAAAJ">Google Scholar</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/rachel0/">LinkedIn</a> &nbsp/&nbsp
                <a href="https://github.com/rachel-1/">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/headshot.JPG"><img style="width:100%;max-width:100%" alt="profile photo" src="images/headshot_circle.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I'm interested in robotics, especially multimodal perception.
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
	      <img style="width:100%;max-width:100%" alt="wiping task" src="images/vices.jpg">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://stanfordvl.github.io/vices/">
                <papertitle>Variable Impedance Control in End-Effector Space: An Action Space for Reinforcement Learning in Contact-Rich Tasks</papertitle>
              </a>
              <br>
              
              <a href="https://robertomartinmartin.com/">Roberto Martin-Martin</a>,
              <a href="http://stanford.edu/~mishlee/">Michelle Lee</a>,
              <strong>Rachel Gardner</strong>,
              <a href="https://profiles.stanford.edu/silvio-savarese">Silvio Savarese</a>,
              <a href="https://web.stanford.edu/~bohg/">Jeannette Bohg</a>,
              <a href="https://animesh.garg.tech/">Animesh Garg</a>
              <br>
              <em>IROS</em>, 2019  
              <br>
              <a href="https://stanfordvl.github.io/vices/">project page</a> /
              <a href="https://arxiv.org/abs/1906.08880">arXiv</a> /
	      <a href="https://github.com/StanfordVL/robosuite/tree/vices_iros19">code</a> /
              <a href="https://www.youtube.com/watch?v=AozIUIW3Ghs">video</a>
              <p></p>
              <p>When doing reinforcement learning, using actions in end-effector space and allowing the agent to control its own impedance parameters results in better generalizability across robots and better sample efficiency.
              </p>
            </td>
          </tr>
	  
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
	      <img style="width:100%;max-width:100%" alt="BERT model diagram" src="images/qa-plausibility.png">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2011.04883">
                <papertitle>Determining Question-Answer Plausibility in Crowdsourced Datasets Using Multi-Task Learning</papertitle>
              </a>
              <br>
              <strong>Rachel Gardner</strong>,
	      Maya Varma, Clare Zhu,
              <a href="http://www.ranjaykrishna.com/">Ranjay Krishna</a>
              <br>
              <em>EMNLP Workshop on Noisy User-Generated Text</em>, 2020  
              <br>
              <a href="https://arxiv.org/abs/2011.04883">arXiv</a> /
	      <a href="https://github.com/rachel-1/qa_plausibility">code</a>
              <p></p>
              <p>Given (possibly bot-generated) questions and natural language user responses, we can estimate the plausibility of the question and answer pair in order to filter the data to create large-scale datasets and enable the creation of active learning agents.
              </p>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
	      <img style="width:100%;max-width:100%" alt="radiographs with highlighted abnormalities" src="images/nature.png">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www-nature-com.stanford.idm.oclc.org/articles/s42256-019-0126-0">
                <papertitle>Automated abnormality detection in lower extremity radiographs using deep learning</papertitle>
              </a>
              <br>
	      Maya Varma, Mandy Lu*, <strong>Rachel Gardner*</strong>,
	      Jared Dunnmon, Nishith Khandwala,
	      Pranav Rajpurkar, Jin Long,
	      Christopher Beaulieu, Katie Shpanskaya,
	      Fei-Fei Li,
	      Matthew P. Lungren*,
	      Bhavik N. Patel*
              <br>
              <em>Nature Machine Intelligence</em>, 2019  
              <br>
              <a href="https://www-nature-com.stanford.idm.oclc.org/articles/s42256-019-0126-0">Nature</a> /
	      <a href="https://github.com/maya124/MSK-LE">code</a> /
	      <a href="https://aimi.stanford.edu/lera-lower-extremity-radiographs-2">data</a>
              <p></p>
              <p>A single CNN model can be achieve high accuracy for abnormality classification in radiographs even across body parts.
              </p>
            </td>
          </tr> 
        </tbody></table>
	<p style="text-align:right">Website format from <a href="https://jonbarron.info/">Jon Barron</a>.</p>
</body>

</html>
